{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9563af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ALARM_FILE = \"alarm.viewer.F4.09.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc0a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load the file\n",
    "df = pd.read_excel(ALARM_FILE, engine='openpyxl')\n",
    "\n",
    "# 2) Keep only rows with '313PT0' in TagName\n",
    "alarm = df[df['TagName'].str.contains('313PT0', na=False)].copy()\n",
    "\n",
    "# 3) Keep only HiHi/LoLo limit alarms\n",
    "alarm = alarm[alarm['MessageText']\n",
    "              .str.contains(r'HiHi level|LoLo level', regex=True, na=False)].copy()\n",
    "\n",
    "# 4) Keep only the two columns we care about\n",
    "alarm = alarm[['DateTime', 'TagName']].copy()\n",
    "\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "\n",
    "# ... after steps 1–4, before you split TagName …\n",
    "\n",
    "def robust_parse(x):\n",
    "    if pd.isna(x):\n",
    "        return pd.NaT\n",
    "    # If Excel gave you a numeric serial date:\n",
    "    if isinstance(x, (int, float)):\n",
    "        # Excel’s “day 1” is 1899-12-31 (but pandas uses 1899-12-30 as origin)\n",
    "        return pd.to_datetime('1899-12-30') + pd.to_timedelta(x, unit='D')\n",
    "    # Otherwise parse any string, day-first, with or without ms\n",
    "    return parser.parse(str(x), dayfirst=True)\n",
    "\n",
    "alarm['DateTime'] = alarm['DateTime'].apply(robust_parse)\n",
    "\n",
    "\n",
    "# 6) Split TagName into 'asset' and 'alarm'\n",
    "parts = alarm['TagName'].str.split('.', expand=True)\n",
    "alarm['asset'] = parts[0] + '.' + parts[1]\n",
    "alarm['alarm'] = parts[2]\n",
    "\n",
    "# 7) Reorder and reset index\n",
    "alarm = alarm[['DateTime', 'asset', 'alarm']].reset_index(drop=True)\n",
    "\n",
    "print(alarm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ab1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the digits after '313PT0' and convert to int\n",
    "alarm['asset'] = alarm['asset'].str.extract(r'313PT0(\\d+)', expand=False).astype(int)\n",
    "print(alarm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96097cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# If not already datetime dtype:\n",
    "alarm['DateTime'] = pd.to_datetime(alarm['DateTime'])\n",
    "\n",
    "# Round up to nearest 5 seconds\n",
    "alarm['DateTime'] = alarm['DateTime'].dt.ceil('5S')\n",
    "print(alarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61810dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import time\n",
    "from datetime import datetime\n",
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load configuration from input_config.ini\n",
    "config = configparser.ConfigParser()\n",
    "config.read('input_data.ini')\n",
    "\n",
    "# Define tags and query settings to be fetched from config file\n",
    "tags = config.get('QUERY', 'tags').split(', ')\n",
    "start_datetime = config.get('QUERY', 'start_datetime')\n",
    "end_datetime = config.get('QUERY', 'end_datetime')\n",
    "# (csv_directory and csv_filename are no longer strictly needed)\n",
    "csv_directory = config.get('OUTPUT', 'csv_directory')\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_filename = os.path.join(csv_directory, f\"output_data_{current_time}.csv\")\n",
    "\n",
    "# Define query settings\n",
    "ww_retrieval_mode = config.get('QUERY_SETTINGS', 'ww_retrieval_mode', fallback='Cyclic')\n",
    "ww_resolution = config.getint('QUERY_SETTINGS', 'ww_resolution', fallback=10000)\n",
    "ww_quality_rule = config.get('QUERY_SETTINGS', 'ww_quality_rule', fallback='Extended')\n",
    "ww_version = config.get('QUERY_SETTINGS', 'ww_version', fallback='Latest')\n",
    "production_mode_threshold = config.getint('QUERY_SETTINGS', 'production_mode_threshold', fallback=5)\n",
    "\n",
    "def fetch_data(tags, start_datetime, end_datetime):\n",
    "    try:\n",
    "        conn_str = (\n",
    "            r'Driver={SQL Server};'\n",
    "            r'Server=IJMHISDBS03.EDIS.TATASTEEL.COM;'\n",
    "            r'Database=Runtime;'\n",
    "            r'Trusted_Connection=yes;'\n",
    "        )\n",
    "        connection = pyodbc.connect(conn_str)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        print('Fetching data...')\n",
    "\n",
    "        tags_str = \", \".join(f\"[{tag}]\" for tag in tags)\n",
    "        query = (\n",
    "            f\"SET QUOTED_IDENTIFIER OFF \"\n",
    "            f\"SELECT * FROM OPENQUERY(INSQL, \\\"SELECT \"\n",
    "            f\"DateTime = convert(nvarchar, DateTime, 21), \"\n",
    "            f\"{tags_str} \"\n",
    "            f\"FROM WideHistory \"\n",
    "            f\"WHERE wwRetrievalMode = '{ww_retrieval_mode}' \"\n",
    "            f\" AND wwResolution = {ww_resolution} \"\n",
    "            f\" AND wwQualityRule = '{ww_quality_rule}' \"\n",
    "            f\"AND wwVersion = '{ww_version}' \"\n",
    "            f\"AND DateTime >= '{start_datetime}' \"\n",
    "            f\"AND DateTime <= '{end_datetime}' \"\n",
    "            f\"AND [PlantInformation.ProductionMode] >= {production_mode_threshold} \\\" )\"\n",
    "        )\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        while True:\n",
    "            data_chunk = cursor.fetchmany(1000)  # Fetch 1,000 rows at a time\n",
    "            if not data_chunk:\n",
    "                break\n",
    "            yield data_chunk\n",
    "            print(f\"Fetched {len(data_chunk)} records...\")\n",
    "        cursor.close()\n",
    "\n",
    "    except pyodbc.Error as e:\n",
    "        print(f\"Failed to fetch data:\\n{str(e)}\")\n",
    "        return\n",
    "\n",
    "def main():\n",
    "    all_rows = []\n",
    "    # Fetch data in chunks and accumulate into a list\n",
    "    for data_chunk in fetch_data(tags, start_datetime, end_datetime):\n",
    "        if data_chunk:\n",
    "            all_rows.extend(data_chunk)\n",
    "    # Build DataFrame, converting each pyodbc.Row into columns correctly\n",
    "    trend_ = pd.DataFrame.from_records(all_rows, columns=[\"DateTime\"] + tags)\n",
    "    return trend_\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trend_ = main()\n",
    "    print(trend_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c90467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read your original CSV\n",
    "df = pd.read_csv(TREND_FILE)\n",
    "\n",
    "# 2. Specify which column(s) to keep fixed (ID variables).\n",
    "#    Here we assume the first column is your timestamp or key.\n",
    "id_vars = [df.columns[0]]\n",
    "\n",
    "# 3. Melt (unpivot) everything else into two columns: 'Variable' and 'Value'\n",
    "trend = trend_.melt(\n",
    "    id_vars=id_vars,\n",
    "    var_name='Variable',\n",
    "    value_name='Value'\n",
    ")\n",
    "\n",
    "# 4. (Optional) Reorder or rename columns\n",
    "#    e.g., df_unpivoted = df_unpivoted[['Timestamp','Variable','Value']]\n",
    "\n",
    "# 5. Save the result\n",
    "trend.to_csv('historian.db.F5.03_unpivot.csv', index=False)\n",
    "\n",
    "# 6. Quick check\n",
    "print(trend.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be3f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns as requested\n",
    "trend.rename(\n",
    "    columns={\n",
    "        'Variable': 'asset',\n",
    "        'Value': 'valve pos'\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Verify the rename\n",
    "print(trend.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d04e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `trend` exists with an 'asset' column like '313XV021.Status'\n",
    "trend['asset'] = trend['asset'] \\\n",
    "    .str.extract(r'(\\d+)\\.Status') \\\n",
    "    .astype(int) - 20\n",
    "\n",
    "# Verify the transformation\n",
    "print(trend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ef26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ensure DateTime columns are proper datetime dtype\n",
    "trend['DateTime'] = pd.to_datetime(trend['DateTime'])\n",
    "alarm['DateTime'] = pd.to_datetime(alarm['DateTime'])\n",
    "\n",
    "# 2. Merge on both 'asset' and 'DateTime' (left join to keep all trend rows)\n",
    "merged_df = pd.merge(\n",
    "    trend,\n",
    "    alarm,\n",
    "    on=['asset', 'DateTime'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 3. (Optional) Fill non-matching alarm entries if desired\n",
    "merged_df['alarm'] = merged_df['alarm']\n",
    "\n",
    "# 4. Quick check\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d582fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming you’ve already done: import pandas as pd\n",
    "\n",
    "# Map 'HiHi' → 1 and 'LoLo' → -1, overwriting the existing column\n",
    "merged_df['alarm'] = merged_df['alarm'].map({'HiHi': 1, 'LoLo': -1}).fillna(0)\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ensure DateTime is a datetime dtype and df is sorted\n",
    "merged_df['DateTime'] = pd.to_datetime(merged_df['DateTime'])\n",
    "merged_df = merged_df.sort_values(['asset', 'DateTime'])\n",
    "\n",
    "# prepare the output column\n",
    "merged_df['failure_period_HiHi'] = 0\n",
    "\n",
    "# work asset by asset\n",
    "for asset in merged_df['asset'].unique():\n",
    "    idx = merged_df['asset'] == asset\n",
    "    group = merged_df.loc[idx]\n",
    "    \n",
    "    ts    = group['DateTime'].values\n",
    "    alarm = group['alarm'].values\n",
    "    valve = group['valve pos'].values\n",
    "\n",
    "    # precompute “time since start of current valve==2 run”\n",
    "    is_v2 = (valve == 2)\n",
    "    # every time valve≠2 we bump the run-id\n",
    "    run2 = (~is_v2).cumsum()\n",
    "    # first timestamp of each run\n",
    "    first2 = pd.Series(ts).groupby(run2).transform('first').values\n",
    "    since2 = (pd.Series(ts) - first2).dt.total_seconds().values\n",
    "\n",
    "    in_failure = False\n",
    "    # iterate in chronological order\n",
    "    for i, loc in enumerate(group.index):\n",
    "        if not in_failure:\n",
    "            # 1) look for alarm==1, then valve==1 within next 5 s\n",
    "            if alarm[i] == 1:\n",
    "                t0 = ts[i]\n",
    "                # boolean mask of rows within (t0, t0+5s] with valve==1\n",
    "                # we can limit to the next few seconds since data is 1 Hz\n",
    "                mask = (ts > t0) & (ts <= t0 + np.timedelta64(5, 's')) & (valve == 1)\n",
    "                if mask.any():\n",
    "                    in_failure = True\n",
    "                    merged_df.at[loc, 'failure_period_HiHi'] = 1\n",
    "        else:\n",
    "            # 2) we’re in a failure—check for shutdown condition\n",
    "            #    “valve==2 continuously for ≥10 s” resets failure\n",
    "            if is_v2[i] and (since2[i] >= 10):\n",
    "                in_failure = False\n",
    "                # leave this row at 0\n",
    "            else:\n",
    "                merged_df.at[loc, 'failure_period_HiHi'] = 1\n",
    "\n",
    "# merged_df is still sorted by ['asset','DateTime'], with the new column added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ed2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "\n",
    "# helper to catch serials and varied strings\n",
    "def robust_parse(x):\n",
    "    if pd.isna(x):\n",
    "        return pd.NaT\n",
    "    # Excel serial date → Timestamp\n",
    "    if isinstance(x, (int, float)):\n",
    "        # pandas uses 1899-12-30 as day 0\n",
    "        return pd.to_datetime('1899-12-30') + pd.to_timedelta(x, unit='D')\n",
    "    # otherwise parse with day-first\n",
    "    return parser.parse(str(x), dayfirst=True)\n",
    "\n",
    "# 1) Load the file\n",
    "df = pd.read_excel(ALARM_FILE, engine='openpyxl')\n",
    "\n",
    "# 2) Keep only rows with '140M0' in TagName\n",
    "alarm = df[df['TagName'].str.contains('140M0', na=False)].copy()\n",
    "\n",
    "# 3) Keep only Drive alarms\n",
    "alarm = alarm[\n",
    "    alarm['MessageText']\n",
    "         .str.contains(r'Drive alarm', regex=True, na=False)\n",
    "].copy()\n",
    "\n",
    "# 4) Keep only the two columns we care about\n",
    "alarm = alarm[['DateTime', 'TagName']].copy()\n",
    "\n",
    "# 4b) If there were no alarms, give ourselves an empty frame with the right columns\n",
    "if alarm.empty:\n",
    "    alarm = pd.DataFrame({\n",
    "        'DateTime':        pd.Series(dtype='datetime64[ns]'),\n",
    "        'asset':           pd.Series(dtype='object'),\n",
    "        'drive warning':   pd.Series(dtype='int8'),\n",
    "    })\n",
    "else:\n",
    "    # 5) Robustly parse any format (serial, with or without ms) then floor to seconds\n",
    "    alarm['DateTime'] = (\n",
    "        alarm['DateTime']\n",
    "             .apply(robust_parse)\n",
    "             .dt.floor('S')\n",
    "    )\n",
    "\n",
    "    # 6) Split out the asset, set the flag\n",
    "    parts = alarm['TagName'].str.split('.', expand=True)\n",
    "    alarm['asset'] = parts[0]\n",
    "    alarm['drive warning'] = 1\n",
    "\n",
    "    # 7) Keep only the bits we want\n",
    "    alarm = alarm[['DateTime', 'asset', 'drive warning']]\n",
    "\n",
    "# 8) Reset index in any case\n",
    "alarm = alarm.reset_index(drop=True)\n",
    "\n",
    "print(alarm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492954db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the digits after '313PT0' and convert to int\n",
    "alarm['asset'] = alarm['asset'].str.extract(r'140M0(\\d+)', expand=False).astype(int)\n",
    "print(alarm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987469f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ensure DateTime columns are proper datetime dtype\n",
    "merged_df['DateTime'] = pd.to_datetime(merged_df['DateTime'])\n",
    "alarm['DateTime'] = pd.to_datetime(alarm['DateTime'])\n",
    "\n",
    "# 2. Merge on both 'asset' and 'DateTime' (left join to keep all trend rows)\n",
    "merged_merged_df = pd.merge(\n",
    "    merged_df,\n",
    "    alarm,\n",
    "    on=['asset', 'DateTime'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "merged_merged_df['drive warning'] = merged_merged_df['drive warning'].fillna(0)\n",
    "\n",
    "# 4. Quick check\n",
    "print(merged_merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_merged_df.to_csv('merged_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of 1’s in the ‘failure_period_HiHi’ column\n",
    "count_ones = (merged_merged_df['alarm'] == 1).sum()\n",
    "print(f\"Number of 1’s in failure_period_HiHi: {count_ones}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41def9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Work on a copy of your existing DataFrame\n",
    "df = merged_merged_df.copy()\n",
    "\n",
    "# 1) Parse your time column (adjust name if needed)\n",
    "time_col = \"DateTime\"  # or \"time\", \"date\", etc.\n",
    "df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "# 2) Sort & group by asset\n",
    "df = df.sort_values([time_col, \"asset\"])\n",
    "grp = df.groupby(\"asset\")\n",
    "\n",
    "# 3) Helper to count transitions A→B\n",
    "def count_transitions(s, A, B):\n",
    "    return ((s.shift(1) == A) & (s == B)).sum()\n",
    "\n",
    "# 4) Compute counts per asset\n",
    "high_pressure = grp[\"failure_period_HiHi\"]\\\n",
    "    .apply(lambda s: count_transitions(s, 0, 1))\\\n",
    "    .rename(\"High pressure failures\")\n",
    "\n",
    "low_pressure = grp[\"alarm\"]\\\n",
    "    .apply(lambda s: count_transitions(s, 0, -1))\\\n",
    "    .rename(\"Low pressure failures\")\n",
    "\n",
    "rotary_feeder = grp[\"drive warning\"]\\\n",
    "    .apply(lambda s: count_transitions(s, 0, 1))\\\n",
    "    .rename(\"Rotary feeder failures\")\n",
    "\n",
    "# 5) Compute minutes from first 0→1 to next return-to-0 in failure_period_HiHi\n",
    "def first_failure_duration(df_asset):\n",
    "    ser = df_asset.sort_values(time_col)[\"failure_period_HiHi\"]\n",
    "    times = df_asset.sort_values(time_col)[time_col]\n",
    "    # find first 0→1\n",
    "    mask_start = (ser.shift(1) == 0) & (ser == 1)\n",
    "    if not mask_start.any():\n",
    "        return pd.NA\n",
    "    start_idx = mask_start.idxmax()\n",
    "    start_time = times.loc[start_idx]\n",
    "    # find next 0 after that\n",
    "    post = ser.loc[start_idx+1:]\n",
    "    zeros = post[post == 0]\n",
    "    if zeros.empty:\n",
    "        return pd.NA\n",
    "    end_idx = zeros.index[0]\n",
    "    end_time = times.loc[end_idx]\n",
    "    return (end_time - start_time).total_seconds() / 60.0\n",
    "\n",
    "durations = grp.apply(first_failure_duration)\\\n",
    "               .rename(\"failure time HiHi injector (min)\")\n",
    "\n",
    "# 6) Combine into summary DataFrame\n",
    "summary = pd.concat([\n",
    "    high_pressure,\n",
    "    low_pressure,\n",
    "    rotary_feeder,\n",
    "    durations\n",
    "], axis=1).reset_index()\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f656ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# summary is the DataFrame you computed\n",
    "display(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b481fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to Excel\n",
    "summary.to_excel(\"failures_summary.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
